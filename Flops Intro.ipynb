{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a52f0f3",
   "metadata": {},
   "source": [
    "# What is Floating-Point Operations Per Second (FLOPS)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d54d29",
   "metadata": {},
   "source": [
    "### It is a major factor in the comparison of the computational power of different systems, especially in those, where numerical calculations are a key point. This article aims to give a basic understanding of what FLOPS is, the importance of FLOPS as well as explore how FLOPS affects computer performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04af63",
   "metadata": {},
   "source": [
    "### A floating-point operation means that the arithmetic mathematical computation is accomplished on floating-point numbers that may include addition, subtraction, multiplication, or division. Floating point numbers are a method for the representation of real numbers with fraction parts, making it possible to maintain a high degree of accuracy in calculations of scientific and other applications where the use of exact numbers is required. As compared to integer operations, floating-point operations are capable of handling a much broader spectrum of values and may represent enormous or incredibly small numbers depending upon the specific task that is to be performed therefore they are much more suitable for tasks that require a very large amount of computations than the integer operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e784e9",
   "metadata": {},
   "source": [
    "### FLOPS is the ability of a computer to perform calculations especially those of floating point forms and is typically used in science-oriented computations. It measures the number of such operations that the system can execute in terms of one-second computation power. The FLOPS denotes have been taken, where a higher FLOPS means a system that has a greater capability to perform a large number of calculations over a given period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befdc750",
   "metadata": {},
   "source": [
    "# Understanding and Estimating FLOPs in Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44011eb6",
   "metadata": {},
   "source": [
    "### In deep learning, FLOPs (Floating Point Operations) are a key metric for measuring the computational cost of a model. FLOPs represent the number of arithmetic operations (like multiplications and additions) required to make a forward pass through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ae2b7d",
   "metadata": {},
   "source": [
    "# Why is this important?\n",
    "\n",
    "### Performance optimization: Knowing the FLOPs helps assess the efficiency of a model, especially when deploying to resource-constrained environments (e.g., mobile devices or edge computing).\n",
    "\n",
    "### Comparing architectures: Two models might have similar accuracy, but one might require far fewer FLOPs, making it preferable for production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799732ee",
   "metadata": {},
   "source": [
    "# üß† What This Notebook Does\n",
    "\n",
    "### In this notebook, we calculate the FLOPs of a simple feedforward neural network using Keras. We:\n",
    "\n",
    "### Iterate through each layer of the model\n",
    "\n",
    "### Identify the number of operations based on the layer type and its input/output dimensions\n",
    "\n",
    "### Account for both core computations and activation function costs\n",
    "\n",
    "### Summarize the total FLOPs as an estimate of model complexity\n",
    "\n",
    "### This serves as both a learning tool and a practical method to benchmark model efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3ae98",
   "metadata": {},
   "source": [
    "# Import the libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f9efea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271024f3",
   "metadata": {},
   "source": [
    "# Loading the Data and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2ca91e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28), (60000,), (10000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90601246",
   "metadata": {},
   "source": [
    "Raw MNIST pixel values range from 0 to 255, which can cause large gradients and unstable training if not scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00db1ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to [0, 1] and reshape\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5904508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6836f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train_full into train and val\n",
    "x_val = x_train[50000:]\n",
    "y_val = y_train[50000:]\n",
    "\n",
    "x_train = x_train[:50000]\n",
    "y_train = y_train[:50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c9f53e",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9fc012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c8b0c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7769dba8",
   "metadata": {},
   "source": [
    "# Flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fa851ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flops(model):\n",
    "    total_flops = 0\n",
    "\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if isinstance(layer, tf.keras.layers.Flatten):\n",
    "            print(f'Layer {i}: Flatten ‚Äî no FLOPs counted.')\n",
    "            continue\n",
    "        \n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            print(f'Layer {i}: Dense')\n",
    "            input_units = layer.input_shape[-1]\n",
    "            output_units = layer.units\n",
    "            flops = 2 * input_units * output_units  # Multiply-accumulate\n",
    "            print(f'  input_units: {input_units}, output_units: {output_units}, flops: {flops}')\n",
    "\n",
    "            # Activation function (approximate, optional)\n",
    "            if layer.activation == tf.keras.activations.relu:\n",
    "                flops += output_units  # 1 op per unit\n",
    "                print(f'  +ReLU activation FLOPs: {output_units}')\n",
    "            elif layer.activation == tf.keras.activations.softmax:\n",
    "                flops += 5 * output_units  # exp, sum, divide\n",
    "                print(f'  +Softmax activation FLOPs (approx): {5 * output_units}')\n",
    "            \n",
    "            total_flops += flops\n",
    "\n",
    "    print(f\"\\nTotal estimated FLOPs: {total_flops:,}\")\n",
    "    return total_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd563954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: Flatten ‚Äî no FLOPs counted.\n",
      "Layer 1: Dense\n",
      "  input_units: 784, output_units: 100, flops: 156800\n",
      "  +ReLU activation FLOPs: 100\n",
      "Layer 2: Dense\n",
      "  input_units: 100, output_units: 100, flops: 20000\n",
      "  +ReLU activation FLOPs: 100\n",
      "Layer 3: Dense\n",
      "  input_units: 100, output_units: 10, flops: 2000\n",
      "  +Softmax activation FLOPs (approx): 50\n",
      "\n",
      "Total estimated FLOPs: 179,050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "179050"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flops(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a9885a",
   "metadata": {},
   "source": [
    "### The number 179,050 FLOPs represents the estimated total number of floating point operations needed for one forward pass through your neural network model ‚Äî that is, processing a single input image (e.g., one 28√ó28 MNIST digit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc3edfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.2643 - accuracy: 0.9236 - val_loss: 0.1180 - val_accuracy: 0.9659\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1121 - accuracy: 0.9654 - val_loss: 0.1155 - val_accuracy: 0.9675\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0799 - accuracy: 0.9751 - val_loss: 0.0958 - val_accuracy: 0.9726\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0606 - accuracy: 0.9808 - val_loss: 0.0845 - val_accuracy: 0.9752\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0482 - accuracy: 0.9842 - val_loss: 0.0826 - val_accuracy: 0.9771\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0381 - accuracy: 0.9876 - val_loss: 0.0866 - val_accuracy: 0.9766\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0316 - accuracy: 0.9897 - val_loss: 0.0844 - val_accuracy: 0.9790\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0274 - accuracy: 0.9905 - val_loss: 0.1158 - val_accuracy: 0.9712\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0225 - accuracy: 0.9924 - val_loss: 0.1109 - val_accuracy: 0.9750\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0220 - accuracy: 0.9927 - val_loss: 0.0971 - val_accuracy: 0.9760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1fbc6d47490>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b7dbe0",
   "metadata": {},
   "source": [
    "# ‚ùì Does model.compile(...) affect FLOPs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1ee2ce",
   "metadata": {},
   "source": [
    "### No, model.compile(...) does not affect the forward-pass FLOPs that you're estimating.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe5c991",
   "metadata": {},
   "source": [
    "# Can I still get a good result with reduced number of flops?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b40f224",
   "metadata": {},
   "source": [
    "# Modified Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32c0aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')  \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48d1ef4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: Flatten ‚Äî no FLOPs counted.\n",
      "Layer 1: Dense\n",
      "  input_units: 784, output_units: 50, flops: 78400\n",
      "  +ReLU activation FLOPs: 50\n",
      "Layer 2: Dense\n",
      "  input_units: 50, output_units: 50, flops: 5000\n",
      "  +ReLU activation FLOPs: 50\n",
      "Layer 3: Dense\n",
      "  input_units: 50, output_units: 10, flops: 1000\n",
      "  +Softmax activation FLOPs (approx): 50\n",
      "\n",
      "Total estimated FLOPs: 84,550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "84550"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flops(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cb290a",
   "metadata": {},
   "source": [
    "### This model does 84,550 arithmetic operations (like multiply, add, activate) to process one input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95b5cd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3385 - accuracy: 0.9014 - val_loss: 0.1749 - val_accuracy: 0.9501\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 0.1591 - accuracy: 0.9536 - val_loss: 0.1316 - val_accuracy: 0.9634\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.1151 - accuracy: 0.9658 - val_loss: 0.1226 - val_accuracy: 0.9630\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 0.0915 - accuracy: 0.9725 - val_loss: 0.1081 - val_accuracy: 0.9681\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.0749 - accuracy: 0.9775 - val_loss: 0.0981 - val_accuracy: 0.9711\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 0.0642 - accuracy: 0.9808 - val_loss: 0.0971 - val_accuracy: 0.9720\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.0550 - accuracy: 0.9833 - val_loss: 0.0970 - val_accuracy: 0.9726\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 0.0466 - accuracy: 0.9852 - val_loss: 0.1166 - val_accuracy: 0.9699\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.0421 - accuracy: 0.9871 - val_loss: 0.0969 - val_accuracy: 0.9732\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.0379 - accuracy: 0.9882 - val_loss: 0.1055 - val_accuracy: 0.9732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1fbc9bb6ad0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37552844",
   "metadata": {},
   "source": [
    "### The current flops function provides a basic estimation of the number of floating-point operations (FLOPs) required for the forward pass of a model, specifically handling layers like Dense and Flatten.\n",
    "\n",
    "### However, this function can be modified and extended to better match your model architecture. For example, if your model includes layers such as Conv2D, LSTM, or BatchNormalization, you can add corresponding logic to compute their FLOPs accurately.\n",
    "\n",
    "### You can also refine the calculation to include batch-level FLOPs, activation function costs, or even training-related computations like backpropagation if needed. Customizing the FLOPs function makes it more adaptable and useful for evaluating model complexity, optimizing for deployment, or comparing alternative architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c71fae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
